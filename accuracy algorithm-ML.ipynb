{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "330fa012-5efc-4110-a910-80a0fb05ffd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidfuzz\n",
      "  Downloading rapidfuzz-3.12.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Downloading rapidfuzz-3.12.1-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rapidfuzz\n",
      "Successfully installed rapidfuzz-3.12.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1ea55f56-8473-4b5f-a820-7d3461de51bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from rapidfuzz import fuzz, process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5412a97a-9cc1-4525-9dd3-7120c0b7e7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def word_match_percentage(word_list_str, text, threshold=50):\n",
    "    \"\"\"\n",
    "    Computes the percentage of words from the list that have an exact or close match in the text.\n",
    "\n",
    "    :param word_list_str: Comma-separated words (string).\n",
    "    :param text: The full text to search within.\n",
    "    :param threshold: Similarity threshold (0-100) for fuzzy matching (higher = stricter).\n",
    "    :return: Percentage of words matched.\n",
    "    \"\"\"\n",
    "    # Convert the comma-separated string into a set of words, stripping spaces and lowercasing\n",
    "    word_list = {word.strip().lower() for word in word_list_str.split(\",\")}\n",
    "\n",
    "    # Tokenize the text into words using regex and convert to lowercase\n",
    "    text_words = set(re.findall(r'\\b[a-zA-Z]+\\b', text.lower()))\n",
    "\n",
    "    # Find words in word_list that have an exact or close match in the text\n",
    "    matched_words = set()\n",
    "\n",
    "    for word in word_list:\n",
    "        if word in text_words:  # Exact match\n",
    "            matched_words.add(word)\n",
    "        else:  # Fuzzy match with a stricter threshold\n",
    "            match, score, _ = process.extractOne(word, text_words, scorer=fuzz.ratio)\n",
    "            if score >= threshold:\n",
    "                matched_words.add(word)\n",
    "\n",
    "    # Calculate the percentage\n",
    "    percentage = (len(matched_words) / len(word_list)) * 100 if word_list else 0\n",
    "\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43953f90-fdb0-4825-8967-1efb033ab00a",
   "metadata": {},
   "source": [
    " ### Machine Learning and Optimization\n",
    " ## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "a5b74519-306f-4372-83b6-f2a0dd690038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSeek Percentage of words found: 61.54%\n",
      "ChatGP Percentage of words found: 46.15%\n",
      "Gemini Percentage of words found: 38.46%\n"
     ]
    }
   ],
   "source": [
    "# Machine Learning and Optimization\n",
    "# Question 1\n",
    "word_list_str_Q1_Benchmark = \"\"\"generalized, small training dataset, nonstationary conditions, high degrees of spatial heterogeneity, limited to one or a few test cases, limited number of sites, transferable to other regions, models' lack of physical interpretability, very complicated to comprehend, being hard to interpret, cannot be easily understood, implausible, lack physical consistency, lack of transparency, decision-making, explainable AI, integrating physical processes and machine learning models, small sample size, data availability, data are still scarce, data are expensive or time-consuming to collect, data may have imbalanced classes, sampling bias, low-probability event, information does not necessarily increase linearly with the amount of data, “informativeness” of the data \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "text_Q1_DeepSeek = \"\"\" 1. Data Scarcity, Heterogeneity, and Quality\n",
    "Hydrologic systems are inherently complex, with processes operating across diverse spatial and temporal scales. While data availability has improved, critical gaps persist:\n",
    "•\tSpatial and Temporal Sparsity: Remote or understudied regions (e.g., high-altitude catchments, arid zones) lack sufficient monitoring infrastructure, leading to sparse data. Temporal gaps in time series (e.g., missing precipitation records during extreme events) further degrade model reliability.\n",
    "•\tHeterogeneity in Data Sources: Hydrologic data often come from disparate sources (e.g., satellite remote sensing, ground-based gauges, citizen science), each with varying resolutions, accuracies, and formats. Harmonizing these datasets for ML training remains a challenge.\n",
    "•\tRepresentation of Extreme Events: Rare events like floods or multi-year droughts are underrepresented in training data, causing ML models to underperform during critical scenarios. For example, streamflow models trained on \"normal\" conditions may fail during 100-year flood events.\n",
    "Advancements: Efforts like the CAMELS (Catchment Attributes and Meteorology for Large-sample Studies) dataset aim to standardize hydrologic data across catchments. Techniques such as data fusion (e.g., merging radar rainfall with gauge data) and synthetic data generation (via physics-based simulations) are being explored to mitigate scarcity. \n",
    " \n",
    "2. Lack of Physical Interpretability and Consistency\n",
    "ML models, particularly deep neural networks, often act as \"black boxes,\" limiting their acceptance in hydrology, where physical process understanding is paramount:\n",
    "•\tDisconnect from Physically Based Laws: Traditional hydrologic models (e.g., SWAT, HEC-HMS) enforce mass/energy conservation, while ML models may violate these principles, leading to unrealistic predictions (e.g., negative evapotranspiration).\n",
    "•\tInterpretability Trade-offs: Stakeholders (e.g., water managers) require transparent models to justify decisions. For instance, predicting dam releases using an unexplainable ML model raises operational and regulatory concerns. \n",
    "Advancements: Hybrid physics-informed ML frameworks are emerging. Examples include:\n",
    "•\tPhysics-Guided Neural Networks (PGNNs): Incorporating physical constraints (e.g., soil moisture continuity equations) into loss functions.\n",
    "•\tSymbolic Regression: Using genetic algorithms to derive interpretable equations from data (e.g., discovering parsimonious rainfall-runoff relationships).\n",
    "•\tExplainable AI (XAI): Tools like SHAP (SHapley Additive exPlanations) are being adapted to interpret feature importance in hydrologic predictions.\n",
    " \n",
    "3. Limited Generalization and Transferability Across Systems\n",
    "Hydrologic systems exhibit significant spatial variability (e.g., geology, land cover) and temporal non-stationarity (e.g., climate change), challenging ML's generalizability:\n",
    "•\tCatchment-Specific Biases: Models trained on data from humid, temperate catchments often fail when applied to arid or snow-dominated regions due to differing dominant processes (e.g., infiltration vs. sublimation).\n",
    "•\tNon-Stationarity Under Climate Change: Historical data may not represent future conditions (e.g., shifting precipitation patterns), reducing model robustness. For example, ML-based snowmelt models trained on past warming trends may underestimate melt rates under accelerated climate change.\n",
    "•\tScale Mismatches: ML models trained on small-scale experimental data (e.g., plot-level soil moisture) struggle to extrapolate to watershed or continental scales.\n",
    "Advancements:\n",
    "•\tTransfer Learning: Pre-training models on large, diverse datasets (e.g., global hydrologic models) and fine-tuning for specific regions.\n",
    "•\tDomain Adaptation: Techniques like adversarial training to align feature distributions between source and target catchments.\n",
    "•\tDynamic Embedding Architectures: Models that encode catchment attributes (e.g., soil type, slope) as input features to improve cross-region adaptability.\n",
    " \n",
    "Conclusion\n",
    "These challenges underscore the need for collaborative innovation between ML experts and hydrologists. Emerging paradigms like \"digital twins\" for watersheds and federated learning (to leverage distributed data without compromising privacy) are poised to address these hurdles. However, success hinges on balancing data-driven insights with the immutable laws governing hydrologic systems. \n",
    " \"\"\"\n",
    "\n",
    "\n",
    "text_Q1_ChatGPT = \"\"\" The application of machine learning (ML) in hydrology and other geosciences disciplines has faced three primary challenges:\n",
    "1. Data Quality, Availability, and Representativeness\n",
    "•\tSparse and Incomplete Observations: Hydrologic datasets often have missing values, sparse spatial coverage, and limited temporal resolution due to sensor failures, financial constraints, or logistical difficulties in data collection (e.g., in remote or ungauged basins).\n",
    "•\tHeterogeneous Data Sources: Hydrologic processes involve multiple interdependent variables, including precipitation, temperature, soil moisture, and streamflow, measured at different spatial and temporal scales. Integrating these disparate datasets into ML models is complex.\n",
    "•\tBias and Uncertainty in Observations: Many hydrologic datasets contain systematic biases due to sensor inaccuracies or human-induced changes (e.g., land use modifications, climate change). ML models trained on such biased data may produce unreliable predictions.\n",
    "2. Interpretability and Physical Consistency\n",
    "•\tBlack-Box Nature of ML Models: Many deep learning approaches (e.g., recurrent neural networks, transformers) lack transparency in their decision-making process. This limits trust and adoption in critical hydrologic applications like flood forecasting and water resource management.\n",
    "•\tViolation of Physical Laws: Traditional hydrologic models are governed by fundamental physical principles (e.g., conservation of mass, energy, and momentum), whereas ML models rely purely on data patterns. Without proper constraints, ML models may generate physically inconsistent or unrealistic outputs.\n",
    "•\tGeneralization Across Diverse Hydrologic Conditions: Hydrologic systems exhibit significant variability across regions and climates. ML models trained in one basin may not generalize well to others, limiting their broader applicability.\n",
    "3. Computational and Methodological Challenges \n",
    "•\tHigh Computational Costs: Training ML models, especially deep learning architectures, requires substantial computational resources. Hydrologic simulations with ML often demand high-performance computing (HPC) infrastructure, which may not be readily available to all researchers or institutions.\n",
    "•\tSelection of Optimal ML Algorithms and Features: The performance of ML models depends on appropriate feature engineering, hyperparameter tuning, and model selection. In hydrology, where processes are complex and nonlinear, choosing the most suitable ML approach is not straightforward.\n",
    "•\tLack of Standardized Benchmarking: Unlike other domains (e.g., computer vision or natural language processing), hydrology lacks widely accepted benchmark datasets and evaluation metrics. This makes it difficult to compare ML model performance across studies.\n",
    "Conclusion\n",
    "Addressing these challenges requires interdisciplinary collaboration between hydrologists, data scientists, and ML experts. Integrating physics-based constraints into ML models (hybrid modeling), improving data assimilation techniques, and developing explainable AI (XAI) methods are promising research directions for advancing ML applications in hydrology. \n",
    " \"\"\"\n",
    "\n",
    "text_Q1_Gemini = \"\"\" While machine learning (ML) holds immense potential in hydrology, its application is hindered by three primary challenges:\n",
    "1.\tData Heterogeneity and Scarcity: Hydrological data is inherently diverse, encompassing various formats (time series, spatial grids, point measurements), scales (from local to regional), and sources (satellites, gauges, models). Integrating this heterogeneous data into a cohesive ML framework is complex. Furthermore, despite growing repositories, high-quality, long-term hydrological data is often scarce, limiting the ability of ML models to generalize well and accurately capture complex hydrological processes.\n",
    "2.\tInterpretability and Explainability: Many ML models, particularly deep learning, are often considered \"black boxes,\" providing accurate predictions without clear explanations of the underlying relationships. In hydrology, understanding why a model makes a specific prediction is crucial for building trust, ensuring reliability, and gaining insights into hydrological processes. The lack of interpretability hinders the acceptance and effective use of ML in water resources management, where understanding the drivers of hydrological phenomena is essential.\n",
    "3.\tGeneralization and Transferability: ML models trained on specific datasets or regions often struggle to generalize to unseen conditions or different geographical areas. Hydrological systems are complex and influenced by various factors (climate, geology, land use), making it challenging to develop models that can be readily transferred to new environments. Ensuring that ML models can reliably predict hydrological behavior in diverse and changing conditions remains a significant challenge.\n",
    "Addressing these challenges requires interdisciplinary collaboration between hydrologists, computer scientists, and data scientists. Future research should focus on developing robust ML frameworks that can handle data heterogeneity, incorporate domain knowledge for interpretability, and improve generalization capabilities for reliable hydrological predictions. \n",
    " \"\"\"\n",
    "\n",
    "\n",
    "percentage_Q1_DeepSeek = word_match_percentage(word_list_str_Q1_Benchmark, text_Q1_DeepSeek)\n",
    "percentage_Q1_ChatGPT  = word_match_percentage(word_list_str_Q1_Benchmark, text_Q1_ChatGPT )\n",
    "percentage_Q1_Gemini = word_match_percentage(word_list_str_Q1_Benchmark, text_Q1_Gemini)\n",
    "\n",
    "\n",
    "\n",
    "print(f\"DeepSeek Percentage of words found: {percentage_Q1_DeepSeek:.2f}%\")\n",
    "print(f\"ChatGP Percentage of words found: {percentage_Q1_ChatGPT:.2f}%\")\n",
    "print(f\"Gemini Percentage of words found: {percentage_Q1_Gemini:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c547740f-a50f-497b-9e08-f293a12742ae",
   "metadata": {},
   "source": [
  
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e12be30-39dd-4b2f-af9b-794c024a10c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
